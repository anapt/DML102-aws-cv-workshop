{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect and blur faces in video using Amazon Rekognition Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "This notebook provides a walkthrough of [face detection API](https://docs.aws.amazon.com/rekognition/latest/APIReference/API_StartFaceDetection.html) in Amazon Rekognition Video to identify faces in a stored video.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "Installing required libraries:   \n",
    "-**opencv-python** : pre-built CPU-only OpenCV packages for Python. OpenCV is a library of programming functions mainly aimed at real-time computer vision.  \n",
    "-**ffmpeg-python** : python bindings for FFmpeg. FFmpeg is a complete, cross-platform solution to record, convert and stream audio and video.   \n",
    "-**moviepy** : a Python library for video editing: cutting, concatenations, title insertions, video compositing (a.k.a. non-linear editing), video processing, and creation of custom effects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python boto3 botocore ffmpeg-python moviepy --upgrade -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client(\"sts\")\n",
    "account = client.get_caller_identity()[\"Account\"]\n",
    "print(f\"AWS account:{account}\")\n",
    "\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "print(f\"AWS region:{region}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 bucket that will be the location from where we will read images/videos\n",
    "\n",
    "bucket = 'dml102-20221003-sagemaker-bucket-{region}-{account}'.format(region = region, account = account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy video from local EBS storage to S3 bucket\n",
    "\n",
    "!aws s3 cp videos/people-walking.mp4 s3://$bucket/rekognition-video-demo/people-walking.mp4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect faces in video\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the code in blur_faces/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from blur_faces.rekognition import get_timestamps_and_faces\n",
    "from blur_faces.rekognition import boto3_client, start_face_detection\n",
    "from blur_faces.rekognition import wait_for_completion\n",
    "from blur_faces.video_processor import apply_faces_to_video, integrate_audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing local paths to video and local output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = 'rekognition-video-demo/people-walking.mp4'\n",
    "local_path_to_video = 'videos/people-walking.mp4'\n",
    "local_output = 'videos/output.mp4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Calling the [start face detection API](https://docs.aws.amazon.com/rekognition/latest/APIReference/API_StartFaceDetection.html)  \n",
    "This API starts asynchronous detection of faces in a stored video.\n",
    "\n",
    "Amazon Rekognition Video can detect faces in a video stored in an Amazon S3 bucket. When calling the API you need to specify the bucket name and the filename of the video. StartFaceDetection returns a job identifier (JobId) that you use to get the results of the operation.\n",
    "```\n",
    "def start_face_detection(bucket, video, size, reko_client=None):\n",
    "    assert check_format_and_size(video, size)\n",
    "    if reko_client == None:\n",
    "        reko_client = boto3.client('rekognition')\n",
    "    response = reko_client.start_face_detection(Video={'S3Object': {'Bucket': bucket, 'Name': video}})\n",
    "    return response['JobId']\n",
    "```\n",
    "2. Checking the progress of the job, by calling the [get_face_detection(JobId=job_id) API](https://docs.aws.amazon.com/rekognition/latest/APIReference/API_GetFaceDetection.html) every 30sec  \n",
    "Gets face detection results for a Amazon Rekognition Video analysis started by StartFaceDetection.\n",
    "\n",
    "Face detection with Amazon Rekognition Video is an asynchronous operation. You start face detection by calling StartFaceDetection which returns a job identifier (JobId). \n",
    " In this step we are calling the get_face_detection API passing the `JobId` and checking the JobStatus response. This parameter can have one of the following values `IN_PROGRESS | SUCCEEDED | FAILED`\n",
    "\n",
    "When the face detection operation finishes, the JobStatus will change from `IN_PROGRESS` to `SUCCEEDED`.\n",
    "\n",
    "```\n",
    "def wait_for_completion(job_id, wait_time_in_s=30, reko_client=None):\n",
    "    if reko_client == None:\n",
    "        reko_client = boto3.client('rekognition')\n",
    "    response = reko_client.get_face_detection(JobId=job_id)\n",
    "    while (response['JobStatus'] == 'IN_PROGRESS'):\n",
    "        print('.', end='')\n",
    "        time.sleep(wait_time_in_s)\n",
    "        response = reko_client.get_face_detection(JobId=job_id)\n",
    "    print('Complete')\n",
    "    return response  \n",
    "```\n",
    "3. Get timestamps and faces  \n",
    "When the JobStatus is `SUCCEEDED` GetFaceDetection returns an array of detected faces (Faces) sorted by the time the faces were detected.\n",
    "\n",
    "Use MaxResults parameter to limit the number of labels returned. If there are more results than specified in MaxResults, the value of NextToken in the operation response contains a pagination token for getting the next set of results. To get the next page of results, call GetFaceDetection and populate the NextToken request parameter with the token value returned from the previous call to GetFaceDetection.\n",
    "\n",
    "In the `get_timestamps_and_faces` function, we iterate over the results and keep the bounding boxes of all the faces and the timestamps they appear at.\n",
    "\n",
    "```\n",
    "def get_timestamps_and_faces(response, job_id, reko_client=None):\n",
    "    final_timestamps = {}\n",
    "    next_token = \"Y\"\n",
    "    first_round = True\n",
    "    while next_token != \"\":\n",
    "        print('.', end='')\n",
    "        # Set some variables if it's the first iteration\n",
    "        if first_round:\n",
    "            next_token = \"\"\n",
    "            first_round = False\n",
    "        # Query Reko Video\n",
    "        response = reko_client.get_face_detection(JobId=job_id, NextToken=next_token)\n",
    "        # Iterate over every face\n",
    "        for face in response['Faces']:\n",
    "            f = face[\"Face\"][\"BoundingBox\"]\n",
    "            t = str(face[\"Timestamp\"])\n",
    "            time_faces = final_timestamps.get(t)\n",
    "            if time_faces == None:\n",
    "                final_timestamps[t] = []\n",
    "            final_timestamps[t].append(f)\n",
    "        # Check if there is another portion of the response\n",
    "        try:\n",
    "            next_token = response['NextToken']\n",
    "        except:\n",
    "            break\n",
    "    # Return the final dictionary\n",
    "    print('Complete')\n",
    "    return final_timestamps\n",
    "```\n",
    "\n",
    "\n",
    "4. Finally, we apply blurring to the faces detected in the video using OpenCV  \n",
    "Explore the code in `blur_faces/video_processor.py`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reko = boto3_client()\n",
    "job_id = start_face_detection(bucket, video, 1, reko)\n",
    "response = wait_for_completion(job_id, reko_client=reko)\n",
    "timestamps=get_timestamps_and_faces(response, job_id, reko)\n",
    "apply_faces_to_video(timestamps, local_path_to_video, local_output, response[\"VideoMetadata\"])\n",
    "# integrate_audio('videos/video-test.mp4', 'videos/output.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Navigate to the `/videos/` directory and locate the newly created `output.mp4` file. Download the file by right clicking on it and pressing download.  \n",
    "Use a multimedia player in your local machine such as VLC to play the video and notice how the faces are now blurred.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serverless implementation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This task can also be completed entirely serverless-ly, so we don’t need to provision, scale, or maintain our infrastructure. Furthermore, it can also be automated to run every time we have a video uploaded to an S3 bucket.\n",
    "\n",
    "In this solution, AWS Step Functions, a low-code visual workflow service used to orchestrate AWS services, automate business processes, and build serverless applications, is used to orchestrate the calls and manage the flow of data between AWS Lambda functions. When an object is created in an Amazon Simple Storage Service (S3) bucket, for example by a video file upload, an ObjectCreated event is detected and a first Lambda function is triggered. This Lambda function makes an asynchronous call to the Amazon Rekognition Video face detection API and starts the execution of the AWS Step Functions workflow.\n",
    "\n",
    "Inside the workflow, we use a Lambda function and a Wait State until the Amazon Rekognition Video asynchronous analysis started earlier finishes execution. Afterward, another Lambda function retrieves the result of the completed process from Amazon Rekognition and passes it to another Lambda function that uses OpenCV to blur the detected faces. To easily use OpenCV with our Lambda function, we built a Docker image hosted on Amazon Elastic Container Registry (ECR), and then deployed on AWS Lambda thanks to Container Image Support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Serverless architecture](img/blur-faces-arch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The event flow starts at the moment of the video ingestion into Amazon S3. Amazon Rekognition Video supports MPEG-4 and MOV file formats, encoded using the H.264 codec.\n",
    "2. After the video file has been stored into Amazon S3, it automatically kicks-off an event triggering a Lambda function.\n",
    "3. The Lambda function uses the video’s attributes (name and location on Amazon S3) to start the face detection job on Amazon Rekognition through an API call.\n",
    "4. The same Lambda function then starts the Step Functions state machine, forwarding the video’s attributes and the Amazon Rekognition job ID.\n",
    "5. The Step Functions workflow starts with a Lambda function waiting for the Amazon Rekognition job to be finished. Once it’s done, another Lambda function gets the results from Amazon Rekognition.\n",
    "6. Finally, a Lambda function with Container Image Support fetches its Docker image, which supports OpenCV from Amazon ECR, blurs the faces detected by Amazon Rekognition, and temporarily stores the output video locally.\n",
    "7. Then, the blurred video is put into the output S3 bucket and removed from local files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look closer inside the AWS Step Function workflow  \n",
    "![step_functions_workflow](img/step_functions.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
