{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect and blur faces in video using Amazon Rekognition Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "This notebook provides a walkthrough of [face detection API](https://docs.aws.amazon.com/rekognition/latest/APIReference/API_StartFaceDetection.html) in Amazon Rekognition Video to identify faces in a stored video.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "Installing required libraries:   \n",
    "-**opencv-python** : pre-built CPU-only OpenCV packages for Python. OpenCV is a library of programming functions mainly aimed at real-time computer vision.  \n",
    "-**ffmpeg-python** : python bindings for FFmpeg. FFmpeg is a complete, cross-platform solution to record, convert and stream audio and video.   \n",
    "-**moviepy** :a Python library for video editing: cutting, concatenations, title insertions, video compositing (a.k.a. non-linear editing), video processing, and creation of custom effects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python boto3 botocore ffmpeg-python moviepy --upgrade -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client(\"sts\")\n",
    "account = client.get_caller_identity()[\"Account\"]\n",
    "print(f\"AWS account:{account}\")\n",
    "\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "print(f\"AWS region:{region}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 bucket that will be the location from where we will read images/videos\n",
    "\n",
    "bucket = 'dml102-20221003-sagemaker-bucket-{region}-{account}'.format(region = region, account = account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy video from local EBS storage to S3 bucket\n",
    "\n",
    "!aws s3 cp videos/people-walking.mp4 s3://$bucket/rekognition-video-demo/people-walking.mp4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect faces in video\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the code in blur_faces/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from blur_faces.rekognition import get_timestamps_and_faces\n",
    "from blur_faces.rekognition import boto3_client, start_face_detection\n",
    "from blur_faces.rekognition import wait_for_completion\n",
    "from blur_faces.video_processor import apply_faces_to_video, integrate_audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon Rekognition reads the video from an S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = 'rekognition-video-demo/people-walking.mp4'\n",
    "local_path_to_video = 'videos/people-walking.mp4'\n",
    "local_output = 'videos/output.mp4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Calling the start face detection API. Async\n",
    "```\n",
    "def start_face_detection(bucket, video, size, reko_client=None):\n",
    "    assert check_format_and_size(video, size)\n",
    "    if reko_client == None:\n",
    "        reko_client = boto3.client('rekognition')\n",
    "    response = reko_client.start_face_detection(Video={'S3Object': {'Bucket': bucket, 'Name': video}})\n",
    "    return response['JobId']\n",
    "```\n",
    "2. Checking the progress of the job, by calling the get_face_detection(JobId=job_id) API every 30sec\n",
    "```\n",
    "def wait_for_completion(job_id, wait_time_in_s=30, reko_client=None):\n",
    "    if reko_client == None:\n",
    "        reko_client = boto3.client('rekognition')\n",
    "    response = reko_client.get_face_detection(JobId=job_id)\n",
    "    while (response['JobStatus'] == 'IN_PROGRESS'):\n",
    "        print('.', end='')\n",
    "        time.sleep(wait_time_in_s)\n",
    "        response = reko_client.get_face_detection(JobId=job_id)\n",
    "    print('Complete')\n",
    "    return response  \n",
    "```\n",
    "3. get timestamps\n",
    "4. apply faces to video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reko = boto3_client()\n",
    "job_id = start_face_detection(bucket, video, 1, reko)\n",
    "response = wait_for_completion(job_id, reko_client=reko)\n",
    "timestamps=get_timestamps_and_faces(response, job_id, reko)\n",
    "apply_faces_to_video(timestamps, local_path_to_video, local_output, response[\"VideoMetadata\"])\n",
    "# integrate_audio('videos/video-test.mp4', 'videos/output.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Serverless architecture](img/blur-faces-arch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The event flow starts at the moment of the video ingestion into Amazon S3. Amazon Rekognition Video supports MPEG-4 and MOV file formats, encoded using the H.264 codec.\n",
    "2. After the video file has been stored into Amazon S3, it automatically kicks-off an event triggering a Lambda function.\n",
    "3. The Lambda function uses the video’s attributes (name and location on Amazon S3) to start the face detection job on Amazon Rekognition through an API call.\n",
    "4. The same Lambda function then starts the Step Functions state machine, forwarding the video’s attributes and the Amazon Rekognition job ID.\n",
    "5. The Step Functions workflow starts with a Lambda function waiting for the Amazon Rekognition job to be finished. Once it’s done, another Lambda function gets the results from Amazon Rekognition.\n",
    "6. Finally, a Lambda function with Container Image Support fetches its Docker image, which supports OpenCV from Amazon ECR, blurs the faces detected by Amazon Rekognition, and temporarily stores the output video locally.\n",
    "7. Then, the blurred video is put into the output S3 bucket and removed from local files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
