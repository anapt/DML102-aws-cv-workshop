{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 21.1.2 is available.\n",
      "You should consider upgrading via the '/usr/local/opt/python@3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python boto3 botocore ffmpeg-python moviepy --upgrade -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile rekognition.py\n",
    "\n",
    "import boto3\n",
    "import botocore\n",
    "import time\n",
    "\n",
    "def boto3_client():\n",
    "    return boto3.client('rekognition')\n",
    "\n",
    "def check_format_and_size(filename, size):\n",
    "    if filename.split('.')[-1] in ['mp4', 'mov']:\n",
    "        if size < 10*1024*1024*1024:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def start_face_detection(bucket, video, size, reko_client=None):\n",
    "    assert check_format_and_size(video, size)\n",
    "    if reko_client == None:\n",
    "        reko_client = boto3.client('rekognition')\n",
    "    response = reko_client.start_face_detection(Video={'S3Object': {'Bucket': bucket, 'Name': video}})\n",
    "    return response['JobId']\n",
    "\n",
    "def wait_for_completion(job_id, wait_time_in_s=30, reko_client=None):\n",
    "    if reko_client == None:\n",
    "        reko_client = boto3.client('rekognition')\n",
    "    response = reko_client.get_face_detection(JobId=job_id)\n",
    "    while (response['JobStatus'] == 'IN_PROGRESS'):\n",
    "        print('.', end='')\n",
    "        time.sleep(wait_time_in_s)\n",
    "        response = reko_client.get_face_detection(JobId=job_id)\n",
    "    print('Complete')\n",
    "    return response\n",
    "\n",
    "def get_timestamps_and_faces(response, job_id, reko_client=None):\n",
    "    final_timestamps = {}\n",
    "    next_token = \"Y\"\n",
    "    first_round = True\n",
    "    while next_token != \"\":\n",
    "        print('.', end='')\n",
    "        # Set some variables if it's the first iteration\n",
    "        if first_round:\n",
    "            next_token = \"\"\n",
    "            first_round = False\n",
    "        # Query Reko Video\n",
    "        response = reko_client.get_face_detection(JobId=job_id, NextToken=next_token)\n",
    "        # Iterate over every face\n",
    "        for face in response['Faces']:\n",
    "            f = face[\"Face\"][\"BoundingBox\"]\n",
    "            t = str(face[\"Timestamp\"])\n",
    "            time_faces = final_timestamps.get(t)\n",
    "            if time_faces == None:\n",
    "                final_timestamps[t] = []\n",
    "            final_timestamps[t].append(f)\n",
    "        # Check if there is another portion of the response\n",
    "        try:\n",
    "            next_token = response['NextToken']\n",
    "        except:\n",
    "            break\n",
    "    # Return the final dictionary\n",
    "    print('Complete')\n",
    "    return final_timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting video_processor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile video_processor.py\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from moviepy.editor import *\n",
    "import os\n",
    "\n",
    "def anonymize_face_pixelate(frame, face_x, face_w, face_y, face_h, blocks=10):\n",
    "    image = frame[face_y:face_y+face_h, face_x:face_x+face_w]\n",
    "    # divide the input image into NxN blocks\n",
    "    (h, w) = image.shape[:2]\n",
    "    xSteps = np.linspace(0, w, blocks + 1, dtype=\"int\")\n",
    "    ySteps = np.linspace(0, h, blocks + 1, dtype=\"int\")\n",
    "\n",
    "    # loop over the blocks in both the x and y direction\n",
    "    for i in range(1, len(ySteps)):\n",
    "        for j in range(1, len(xSteps)):\n",
    "            # compute the starting and ending (x, y)-coordinates\n",
    "            # for the current block\n",
    "            startX = xSteps[j - 1]\n",
    "            startY = ySteps[i - 1]\n",
    "            endX = xSteps[j]\n",
    "            endY = ySteps[i]\n",
    "\n",
    "            # extract the ROI using NumPy array slicing, compute the\n",
    "            # mean of the ROI, and then draw a rectangle with the\n",
    "            # mean RGB values over the ROI in the original image\n",
    "            roi = image[startY:endY, startX:endX]\n",
    "            (B, G, R) = [int(x) for x in cv2.mean(roi)[:3]]\n",
    "            cv2.rectangle(image, (startX, startY), (endX, endY),\n",
    "                          (B, G, R), -1)\n",
    "\n",
    "    frame[face_y:face_y+face_h, face_x:face_x+face_w] = image\n",
    "    return frame\n",
    "\n",
    "def apply_faces_to_video(timestamps, local_path_to_video, local_output, video_metadata, color=(255,0,0), thickness=2):\n",
    "    # Extract video info\n",
    "    frame_rate = video_metadata[\"FrameRate\"]\n",
    "    frame_height = video_metadata[\"FrameHeight\"]\n",
    "    frame_width = video_metadata[\"FrameWidth\"]\n",
    "    # Set up support for OpenCV\n",
    "    frame_counter = 0\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    # Create the file pointers\n",
    "    v = cv2.VideoCapture(local_path_to_video)\n",
    "    out = cv2.VideoWriter(\n",
    "        filename=local_output, \n",
    "        fourcc=fourcc, \n",
    "        fps=int(frame_rate), \n",
    "        frameSize=(frame_width, frame_height)\n",
    "    )\n",
    "    # Open the video\n",
    "    while v.isOpened():\n",
    "        # Get frames until available\n",
    "        has_frame, frame = v.read()\n",
    "        if has_frame:\n",
    "            for t in timestamps:\n",
    "                faces = timestamps.get(t)\n",
    "                lower_bound = int(int(t)/1000*int(frame_rate))\n",
    "                upper_bound = int(int(t)/1000*int(frame_rate))+(int(frame_rate)/2)\n",
    "                if (frame_counter >= lower_bound) and (frame_counter <= upper_bound):\n",
    "                    for f in faces:\n",
    "                        x = int(f['Left']*frame_width)\n",
    "                        y = int(f['Top']*frame_height)\n",
    "                        w = int(f['Width']*frame_width)\n",
    "                        h = int(f['Height']*frame_height)\n",
    "                        #frame = cv2.rectangle(frame, (x,y), (x+w,y+h), color, thickness)\n",
    "                        frame = anonymize_face_pixelate(frame, x, w, y, h, 50)\n",
    "            out.write(frame)\n",
    "            frame_counter += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    out.release()\n",
    "    v.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(f\"Complete. {frame_counter} frames were written.\")\n",
    "\n",
    "def integrate_audio(original_video, output_video, audio_path='/tmp/audio.mp3'):\n",
    "    # Extract audio\n",
    "    my_clip = VideoFileClip(original_video)\n",
    "    my_clip.audio.write_audiofile(audio_path)\n",
    "\n",
    "    # Join output video with extracted audio\n",
    "    videoclip = VideoFileClip(output_video)\n",
    "    audioclip = AudioFileClip(audio_path)\n",
    "    new_audioclip = CompositeAudioClip([audioclip])\n",
    "    videoclip.audio = new_audioclip\n",
    "    videoclip.write_videofile(output_video)\n",
    "\n",
    "    # Delete audio\n",
    "    os.remove(audio_path)\n",
    "\n",
    "    print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import urllib\n",
    "\n",
    "import boto3\n",
    "import botocore\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from rekognition import check_format_and_size, start_face_detection, get_timestamps_and_faces\n",
    "from video_processor import apply_faces_to_video\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "reko = boto3.client('rekognition')\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "\n",
    "    for record in event['Records']:\n",
    "\n",
    "        # verify event has reference to S3 object\n",
    "        try:\n",
    "            # get metadata of file uploaded to Amazon S3\n",
    "            bucket = record['s3']['bucket']['name']\n",
    "            key = urllib.parse.unquote_plus(record['s3']['object']['key'])\n",
    "            size = int(record['s3']['object']['size'])\n",
    "            filename = key.split('/')[-1]\n",
    "            local_filename = '/tmp/{}'.format(filename)\n",
    "        except KeyError:\n",
    "            error_message = 'Lambda invoked without S3 event data. Event needs to reference a S3 bucket and object key.'\n",
    "            logger.log(error_message)\n",
    "            continue\n",
    "\n",
    "        # verify file and its size\n",
    "        try:\n",
    "            assert check_format_and_size(filename, size)\n",
    "        except:\n",
    "            error_message = 'Unsupported file type. Amazon Rekognition Video support MOV and MP4 lower than 10 GB in size'\n",
    "            logger.log(error_message)\n",
    "            continue\n",
    "\n",
    "        # download file locally to /tmp retrieve metadata\n",
    "        try:\n",
    "            s3.download_file(bucket, key, local_filename)\n",
    "        except botocore.exceptions.ClientError:\n",
    "            error_message = 'Lambda role does not have permission to call GetObject for the input S3 bucket, or object does not exist.'\n",
    "            logger.log(error_message)\n",
    "            continue\n",
    "\n",
    "        # use Amazon Rekognition to detect faces in image uploaded to Amazon S3\n",
    "        try:\n",
    "            job_id = start_face_detection(bucket, key, 1, reko)\n",
    "            response = wait_for_completion(job_id, reko_client=reko)\n",
    "        except rekognition.exceptions.AccessDeniedException:\n",
    "            error_message = 'Lambda role does not have permission to call DetectFaces in Amazon Rekognition.'\n",
    "            logger.log(error_message)\n",
    "            continue\n",
    "        except rekognition.exceptions.InvalidS3ObjectException:\n",
    "            error_message = 'Unable to get object metadata from S3. Check object key, region and/or access permissions for input S3 bucket.'\n",
    "            logger.log(error_message)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            timestamps=get_timestamps_and_faces(response, job_id, reko)\n",
    "            apply_faces_to_video(timestamps, local_path_to_video, local_output, response[\"VideoMetadata\"])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "        # uploaded modified image to Amazon S3 bucket\n",
    "        try:\n",
    "            s3.upload_file(local_output, bucket, key)\n",
    "        except boto3.exceptions.S3UploadFailedError:\n",
    "            error_message = 'Lambda role does not have permission to call PutObject for the output S3 bucket.'\n",
    "            add_failed(bucket, error_message, failed_records, key)\n",
    "            continue\n",
    "\n",
    "        # clean up /tmp\n",
    "        if os.path.exists(local_filename):\n",
    "            os.remove(local_filename)\n",
    "\n",
    "        successful_records.append({\n",
    "            \"bucket\": bucket,\n",
    "            \"key\": key\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        \"statusCode\": 200,\n",
    "        \"body\": json.dumps(\n",
    "            {\n",
    "                \"cv2_version\": cv2.__version__,\n",
    "                \"failed_records\": failed_records,\n",
    "                \"successful_records\": successful_records\n",
    "            }\n",
    "        )\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..Complete\n",
      "..Complete\n",
      "Complete. 2246 frames were written.\n",
      "chunk:  11%|█         | 183/1653 [00:00<00:00, 1827.51it/s, now=None]MoviePy - Writing audio in /tmp/audio.mp3\n",
      "chunk:   0%|          | 0/1654 [00:00<?, ?it/s, now=None]MoviePy - Done.\n",
      "Moviepy - Building video videos/output.mp4.\n",
      "MoviePy - Writing audio in outputTEMP_MPY_wvf_snd.mp3\n",
      "t:   1%|          | 17/2247 [00:00<00:13, 162.48it/s, now=None]MoviePy - Done.\n",
      "Moviepy - Writing video videos/output.mp4\n",
      "\n",
      "Moviepy - Done !\n",
      "Moviepy - video ready videos/output.mp4\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "from rekognition import boto3_client, start_face_detection, wait_for_completion, get_timestamps_and_faces\n",
    "from video_processor import apply_faces_to_video, integrate_audio\n",
    "\n",
    "bucket = 'datasets-dggallit'\n",
    "video = 'rekognition-video-demo/video-test.mp4'\n",
    "local_path_to_video = 'videos/video-test.mp4'\n",
    "local_output = 'videos/output.mp4'\n",
    "\n",
    "reko = boto3_client()\n",
    "job_id = start_face_detection(bucket, video, 1, reko)\n",
    "response = wait_for_completion(job_id, reko_client=reko)\n",
    "timestamps=get_timestamps_and_faces(response, job_id, reko)\n",
    "apply_faces_to_video(timestamps, local_path_to_video, local_output, response[\"VideoMetadata\"])\n",
    "# integrate_audio('videos/video-test.mp4', 'videos/output.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
